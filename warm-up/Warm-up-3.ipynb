{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose a day of significance to you (e.g., your birthday), and retrieve a 5% sample of the comments posted on this particular day across all 5 years of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = \"2\"\n",
    "day = \"1\"\n",
    "\n",
    "sampleSize = 0.05\n",
    "seed = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "30926243\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "|archived|        author|author_cakeday|author_flair_css_class|author_flair_text|                body|body_html|controversiality|created|created_utc|distinguished|downs|edited|gilded|     id|  link_id|mod_reports|name|parent_id|removal_reason|replies|retrieved_on|saved|score|score_hidden|stickied|      subreddit|subreddit_id|ups|user_reports|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "|    null|distanceformed|          null|                 green|           7/7/17|I need hash tag h...|     null|               0|   null| 1474469708|         null| null| false|     0|d7w2ag1|t3_53tgt2|       null|null|t3_53tgt2|          null|   null|  1475908052| null|    6|        null|   false|weddingplanning|    t5_2rv3t|  6|        null|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import from_json, col\n",
    "conf = SparkConf().setAppName('FirstSpark2').setMaster('Spark')\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json(\"hdfs://orion11:20001/sample_sampled_reddit/\")\n",
    "\n",
    "print(type(df))\n",
    "print(df.count())\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "27303462\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "|archived|        author|author_cakeday|author_flair_css_class|author_flair_text|                body|body_html|controversiality|created|created_utc|distinguished|downs|edited|gilded|     id|  link_id|mod_reports|name|parent_id|removal_reason|replies|retrieved_on|saved|score|score_hidden|stickied|      subreddit|subreddit_id|ups|user_reports|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "|    null|distanceformed|          null|                 green|           7/7/17|I need hash tag h...|     null|               0|   null| 1474469708|         null| null| false|     0|d7w2ag1|t3_53tgt2|       null|null|t3_53tgt2|          null|   null|  1475908052| null|    6|        null|   false|weddingplanning|    t5_2rv3t|  6|        null|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.filter(\n",
    "~(df.body.like('[deleted]'))\n",
    "    & ~(df.body.isNull())\n",
    "    & ~(df.author.like('[deleted]'))\n",
    "    & ~(df.author.like('AutoModerator')) \n",
    "    & ~(df.author.rlike(\"[bB][oO][tT]\"))\n",
    "\n",
    ")\n",
    "\n",
    "print(type(df2))\n",
    "print(df2.count())\n",
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating month and day column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "27303462\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+---+-----+\n",
      "|archived|        author|author_cakeday|author_flair_css_class|author_flair_text|                body|body_html|controversiality|created|created_utc|distinguished|downs|edited|gilded|     id|  link_id|mod_reports|name|parent_id|removal_reason|replies|retrieved_on|saved|score|score_hidden|stickied|      subreddit|subreddit_id|ups|user_reports|day|month|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+---+-----+\n",
      "|    null|distanceformed|          null|                 green|           7/7/17|I need hash tag h...|     null|               0|   null| 1474469708|         null| null| false|     0|d7w2ag1|t3_53tgt2|       null|null|t3_53tgt2|          null|   null|  1475908052| null|    6|        null|   false|weddingplanning|    t5_2rv3t|  6|        null| 21|    9|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+---+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, from_unixtime\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "df3 = (df2\n",
    "      .withColumn(\"day\", dayofmonth(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "      .withColumn(\"month\",month(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "      )\n",
    "\n",
    "print(type(df3))\n",
    "print(df3.count())\n",
    "df3.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  dataset where month = month and day = day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "85816\n",
      "+--------+-----------+--------------+----------------------+-----------------+-------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+----------+--------------+-------+------------+-----+-----+------------+--------+----------+------------+---+------------+---+-----+\n",
      "|archived|     author|author_cakeday|author_flair_css_class|author_flair_text|         body|body_html|controversiality|created|created_utc|distinguished|downs|edited|gilded|     id|  link_id|mod_reports|name| parent_id|removal_reason|replies|retrieved_on|saved|score|score_hidden|stickied| subreddit|subreddit_id|ups|user_reports|day|month|\n",
      "+--------+-----------+--------------+----------------------+-----------------+-------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+----------+--------------+-------+------------+-----+-----+------------+--------+----------+------------+---+------------+---+-----+\n",
      "|    null|trillmatic1|          null|                palace|                 |Fucking Lol'd|     null|               0|   null| 1454313606|         null| null| false|     0|czjibg4|t3_43mbal|       null|null|t1_czji7p8|          null|   null|  1458215073| null|    7|        null|   false|streetwear|    t5_2sgoq|  7|        null|  1|    2|\n",
      "+--------+-----------+--------------+----------------------+-----------------+-------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+----------+--------------+-------+------------+-----+-----+------------+--------+----------+------------+---+------------+---+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.where((df3.day.like(day))&(df3.month.like(month)))\n",
    "# df4 = df3.where(df3.day.like(day)).where(df3.month.like(month))\n",
    "\n",
    "\n",
    "print(type(df4))\n",
    "print(df4.count())\n",
    "df4.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5% sample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "4300\n",
      "+--------+-------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+----------+--------------+-------+------------+-----+-----+------------+--------+--------------+------------+---+------------+---+-----+\n",
      "|archived| author|author_cakeday|author_flair_css_class|author_flair_text|                body|body_html|controversiality|created|created_utc|distinguished|downs|edited|gilded|     id|  link_id|mod_reports|name| parent_id|removal_reason|replies|retrieved_on|saved|score|score_hidden|stickied|     subreddit|subreddit_id|ups|user_reports|day|month|\n",
      "+--------+-------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+----------+--------------+-------+------------+-----+-----+------------+--------+--------------+------------+---+------------+---+-----+\n",
      "|    null|Drowzen|          null|                  null|             null|btw this is reall...|     null|               0|   null| 1454313870|         null| null| false|     0|czjie5o|t3_43lnzt|       null|null|t1_czj6t57|          null|   null|  1458215109| null|    1|        null|   false|summonerschool|    t5_2t9x3|  1|        null|  1|    2|\n",
      "+--------+-------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+----------+--------------+-------+------------+-----+-----+------------+--------+--------------+------------+---+------------+---+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df4.sample(False, sampleSize, seed)\n",
    "\n",
    "print(type(df5))\n",
    "print(df5.count())\n",
    "df5.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
