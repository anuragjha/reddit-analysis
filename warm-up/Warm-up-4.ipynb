{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warm-up\n",
    "#### The number of comments posted per year will likely trend upward over time as more users join Reddit. However, the popularity of some subreddits may increase or decrease over time. Find An example of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "30926243\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "|archived|        author|author_cakeday|author_flair_css_class|author_flair_text|                body|body_html|controversiality|created|created_utc|distinguished|downs|edited|gilded|     id|  link_id|mod_reports|name|parent_id|removal_reason|replies|retrieved_on|saved|score|score_hidden|stickied|      subreddit|subreddit_id|ups|user_reports|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "|    null|distanceformed|          null|                 green|           7/7/17|I need hash tag h...|     null|               0|   null| 1474469708|         null| null| false|     0|d7w2ag1|t3_53tgt2|       null|null|t3_53tgt2|          null|   null|  1475908052| null|    6|        null|   false|weddingplanning|    t5_2rv3t|  6|        null|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import from_json, col\n",
    "conf = SparkConf().setAppName('FirstSpark2').setMaster('Spark')\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json(\"hdfs://orion11:20001/sample_sampled_reddit/\")\n",
    "\n",
    "print(type(df))\n",
    "print(df.count())\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "27303462\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "|archived|        author|author_cakeday|author_flair_css_class|author_flair_text|                body|body_html|controversiality|created|created_utc|distinguished|downs|edited|gilded|     id|  link_id|mod_reports|name|parent_id|removal_reason|replies|retrieved_on|saved|score|score_hidden|stickied|      subreddit|subreddit_id|ups|user_reports|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "|    null|distanceformed|          null|                 green|           7/7/17|I need hash tag h...|     null|               0|   null| 1474469708|         null| null| false|     0|d7w2ag1|t3_53tgt2|       null|null|t3_53tgt2|          null|   null|  1475908052| null|    6|        null|   false|weddingplanning|    t5_2rv3t|  6|        null|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.filter(\n",
    "~(df.body.like('[deleted]'))\n",
    "    & ~(df.body.isNull())\n",
    "    & ~(df.author.like('[deleted]'))\n",
    "    & ~(df.author.like('AutoModerator')) \n",
    "    & ~(df.author.rlike(\"[bB][oO][tT]\"))\n",
    "\n",
    ")\n",
    "\n",
    "print(type(df2))\n",
    "print(df2.count())\n",
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Year Column based on created_utc column in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+----+\n",
      "|archived|        author|author_cakeday|author_flair_css_class|author_flair_text|                body|body_html|controversiality|created|created_utc|distinguished|downs|edited|gilded|     id|  link_id|mod_reports|name|parent_id|removal_reason|replies|retrieved_on|saved|score|score_hidden|stickied|      subreddit|subreddit_id|ups|user_reports|year|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+----+\n",
      "|    null|distanceformed|          null|                 green|           7/7/17|I need hash tag h...|     null|               0|   null| 1474469708|         null| null| false|     0|d7w2ag1|t3_53tgt2|       null|null|t3_53tgt2|          null|   null|  1475908052| null|    6|        null|   false|weddingplanning|    t5_2rv3t|  6|        null|2016|\n",
      "|    null|RetardedTiming|          null|                  null|             null|I'll never unders...|     null|               0|   null| 1474469714|         null| null| false|     0|d7w2am5|t3_53qxob|       null|null|t3_53qxob|          null|   null|  1475908055| null|  -10|        null|   false|           cars|    t5_2qhl2|-10|        null|2016|\n",
      "+--------+--------------+--------------+----------------------+-----------------+--------------------+---------+----------------+-------+-----------+-------------+-----+------+------+-------+---------+-----------+----+---------+--------------+-------+------------+-----+-----+------------+--------+---------------+------------+---+------------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, from_unixtime\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "df3 = (df2\n",
    "      .withColumn(\"year\", year(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "#       .withColumn(\"month\",month(from_unixtime(\"created_utc\").cast(DateType())))\n",
    "      )\n",
    "df3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "187565\n",
      "+---------------+----+------+\n",
      "|      subreddit|year| count|\n",
      "+---------------+----+------+\n",
      "|      AskReddit|2016|507143|\n",
      "|      AskReddit|2015|458307|\n",
      "|      AskReddit|2014|425660|\n",
      "|      AskReddit|2013|400348|\n",
      "|      AskReddit|2012|285988|\n",
      "|       politics|2016|174268|\n",
      "|      AskReddit|2011|140039|\n",
      "|      AskReddit|2017|134778|\n",
      "|          funny|2013|118633|\n",
      "|leagueoflegends|2015|114906|\n",
      "|          funny|2012|107919|\n",
      "|     The_Donald|2016|101635|\n",
      "|          funny|2014|101407|\n",
      "|leagueoflegends|2014| 96791|\n",
      "|           pics|2013| 90135|\n",
      "+---------------+----+------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.groupBy(df3.subreddit, df3.year)\n",
    "df4 = df4.count().orderBy('count', ascending=False)\n",
    "\n",
    "print(type(df4))\n",
    "print(df4.count())\n",
    "df4.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of subreddit where popularity is decreasing\n",
    "\n",
    "we check for subreddit is 'pics', count of comments on 'pics'.\n",
    "\n",
    "Number of comments peak at year 2013 and is on decline since.\n",
    "We see a huge jump in year 2010 when Instagram came around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+\n",
      "|subreddit|year|count|\n",
      "+---------+----+-----+\n",
      "|     pics|2017|17639|\n",
      "|     pics|2016|59963|\n",
      "|     pics|2015|70732|\n",
      "|     pics|2014|75620|\n",
      "|     pics|2013|90135|\n",
      "|     pics|2012|81051|\n",
      "|     pics|2011|70335|\n",
      "|     pics|2010|31765|\n",
      "|     pics|2009| 8109|\n",
      "|     pics|2008| 2945|\n",
      "+---------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df4.where(df4.subreddit.like('pics')).orderBy(df4.year, ascending=False)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of subreddit where popularity is increasing\n",
    "we check for subreddit = 'videos'\n",
    "We see that few years after 'pics' were on rise, comments on 'videos' started incresing .\n",
    "Count of comments saw huge jump in year 2012. Can we attributed to introduction of videos and such in instagram after getting acquired by facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+\n",
      "|subreddit|year|count|\n",
      "+---------+----+-----+\n",
      "|   videos|2017|13191|\n",
      "|   videos|2016|56055|\n",
      "|   videos|2015|61333|\n",
      "|   videos|2014|54883|\n",
      "|   videos|2013|44455|\n",
      "|   videos|2012|31858|\n",
      "|   videos|2011|16403|\n",
      "|   videos|2010| 3078|\n",
      "|   videos|2009| 1066|\n",
      "|   videos|2008|  329|\n",
      "+---------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df4.where(df4.subreddit.like('funny')).orderBy(df4.year, ascending=False)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
